{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.11.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird_y': 300, 'pipe_low': [435.0, 473], 'pipe_high': [435.0, 173], 'is_alive': True}\n",
      "state: {'bird_y': 206, 'pipe_low': [363.0, 451], 'pipe_high': [363.0, 151], 'is_alive': True}, reward: 10, old_score: 1, current_score: 1\n"
     ]
    }
   ],
   "source": [
    "# Example implementation\n",
    "random.seed(0)\n",
    "env = Environment()\n",
    "current_state = env.reset()\n",
    "print(current_state)\n",
    "\n",
    "for _ in range(100):\n",
    "    instruction = (_ % 20 == 0)\n",
    "    next_state, reward = env.game_step(instruction)\n",
    "    if reward != 0: print(f\"state: {next_state}, reward: {reward}, old_score: {env.previous_score}, current_score: {env.current_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 10**(-3)          # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Action space\n",
    "Do Nothing = 0 <br>\n",
    "Jump = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Observation space\n",
    "\n",
    "1. Bird's y-pos\n",
    "2. Next pipe's top and bottom"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Reward\n",
    "\n",
    "Crossing a pipe gains 15 points, crashing looses 1000."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Building the brain of our bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 5  # Observation space's size\n",
    "num_actions = 2\n",
    "\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=num_actions, activation=\"linear\")    \n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=num_actions, activation=\"linear\")\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "from collections import namedtuple\n",
    "\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"is_alive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, is_alive = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    y_targets = rewards + is_alive * gamma * max_qsa\n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    loss = MSE(y_targets, q_values)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Training model progressively\n",
    "\n",
    "## 4.1 - Basic training\n",
    "`GAP_WIDTH = 300` where HEIGHT = 600px <br>\n",
    "`pipe.y_center = HEIGHT // 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -100.00\n",
      "Episode 200 | Total point average of the last 100 episodes: -100.00\n",
      "Episode 300 | Total point average of the last 100 episodes: -98.900\n",
      "Episode 400 | Total point average of the last 100 episodes: -97.90\n",
      "Episode 500 | Total point average of the last 100 episodes: -98.40\n",
      "Episode 600 | Total point average of the last 100 episodes: -98.80\n",
      "Episode 700 | Total point average of the last 100 episodes: -99.60\n",
      "Episode 800 | Total point average of the last 100 episodes: -99.80\n",
      "Episode 900 | Total point average of the last 100 episodes: -99.90\n",
      "Episode 1000 | Total point average of the last 100 episodes: -100.00\n",
      "Episode 1100 | Total point average of the last 100 episodes: -100.00\n",
      "Episode 1200 | Total point average of the last 100 episodes: -100.00\n",
      "Episode 1300 | Total point average of the last 100 episodes: -98.200\n",
      "Episode 1400 | Total point average of the last 100 episodes: -97.50\n",
      "Episode 1500 | Total point average of the last 100 episodes: -96.50\n",
      "Episode 1600 | Total point average of the last 100 episodes: -96.80\n",
      "Episode 1700 | Total point average of the last 100 episodes: -98.00\n",
      "Episode 1800 | Total point average of the last 100 episodes: -97.60\n",
      "Episode 1900 | Total point average of the last 100 episodes: -96.60\n",
      "Episode 2000 | Total point average of the last 100 episodes: -97.40\n",
      "\n",
      "Total Runtime: 246.48 s (4.11 min)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2_000\n",
    "max_num_timesteps = 100_000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state = env.reset()\n",
    "    state_array = np.array([\n",
    "        state[\"bird_y\"], \n",
    "        state[\"pipe_low\"][0], \n",
    "        state[\"pipe_low\"][1], \n",
    "        state[\"pipe_high\"][0], \n",
    "        state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "    is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state_array, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = get_action(q_values, epsilon)  # from utils\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward = env.game_step(action)\n",
    "        next_state_array = np.array([\n",
    "            next_state[\"bird_y\"], \n",
    "            next_state[\"pipe_low\"][0], \n",
    "            next_state[\"pipe_low\"][1], \n",
    "            next_state[\"pipe_high\"][0], \n",
    "            next_state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "        is_alive = int(state[\"is_alive\"])\n",
    "        \n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        memory_buffer.append(experience(state_array, action, reward, next_state_array, is_alive))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)  # from utils\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = get_experiences(memory_buffer)  # from utils\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        state_array = np.array([\n",
    "            state[\"bird_y\"], \n",
    "            state[\"pipe_low\"][0], \n",
    "            state[\"pipe_low\"][1], \n",
    "            state[\"pipe_high\"][0], \n",
    "            state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "        is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "        total_points += reward\n",
    "        \n",
    "        if not is_alive:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 100 points in the last 100 episodes.\n",
    "    if av_latest_points >= 100:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('flappy_bird_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points: 1470\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to the initial state and get the initial state\n",
    "state = env.reset()\n",
    "state_array = np.array([\n",
    "    state[\"bird_y\"], \n",
    "    state[\"pipe_low\"][0], \n",
    "    state[\"pipe_low\"][1], \n",
    "    state[\"pipe_high\"][0], \n",
    "    state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "total_points = 0\n",
    "\n",
    "actions = []\n",
    "\n",
    "for t in range(max_num_timesteps):\n",
    "    # From the current state S choose an action A using an ε-greedy policy\n",
    "    state_qn = np.expand_dims(state_array, axis=0)  # state needs to be the right shape for the q_network\n",
    "    q_values = q_network(state_qn)\n",
    "    action = get_action(q_values, epsilon)  # from utils\n",
    "\n",
    "    actions.append(action)\n",
    "    \n",
    "    # Take action A and receive reward R and the next state S'\n",
    "    next_state, reward = env.game_step(action)\n",
    "    next_state_array = np.array([\n",
    "        next_state[\"bird_y\"], \n",
    "        next_state[\"pipe_low\"][0], \n",
    "        next_state[\"pipe_low\"][1], \n",
    "        next_state[\"pipe_high\"][0], \n",
    "        next_state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "    is_alive = int(state[\"is_alive\"])\n",
    "    \n",
    "    state = next_state.copy()\n",
    "    state_array = np.array([\n",
    "        state[\"bird_y\"], \n",
    "        state[\"pipe_low\"][0], \n",
    "        state[\"pipe_low\"][1], \n",
    "        state[\"pipe_high\"][0], \n",
    "        state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "    is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "    if not is_alive:\n",
    "        break\n",
    "\n",
    "    if reward > 0:\n",
    "        total_points += 1\n",
    "    \n",
    "print(\"points:\", total_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "q_network.save('logs/1.fb_large_width_no_rng.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Randomise height of pipes in a small range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.11.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import namedtuple\n",
    "\n",
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 10**(-3)          # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps\n",
    "\n",
    "env = Environment()\n",
    "\n",
    "state_size = 5  # Observation space's size\n",
    "num_actions = 2\n",
    "\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=num_actions, activation=\"linear\")    \n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=num_actions, activation=\"linear\")\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)\n",
    "\n",
    "q_network = load_model('logs/1.fb_large_width_no_rng.h5')\n",
    "\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"is_alive\"])\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, is_alive = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    y_targets = rewards + is_alive * gamma * max_qsa\n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    loss = MSE(y_targets, q_values)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    update_target_network(q_network, target_q_network)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 200 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 300 | Total point average of the last 100 episodes: -999.100\n",
      "Episode 400 | Total point average of the last 100 episodes: -998.40\n",
      "Episode 500 | Total point average of the last 100 episodes: -994.30\n",
      "Episode 600 | Total point average of the last 100 episodes: -989.30\n",
      "Episode 700 | Total point average of the last 100 episodes: -999.80\n",
      "Episode 800 | Total point average of the last 100 episodes: -999.900\n",
      "Episode 900 | Total point average of the last 100 episodes: -985.70\n",
      "Episode 1000 | Total point average of the last 100 episodes: -999.90\n",
      "Episode 1100 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 1200 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 1300 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 1400 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 1500 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 1600 | Total point average of the last 100 episodes: -1000.00\n",
      "Episode 1700 | Total point average of the last 100 episodes: -973.300\n",
      "Episode 1800 | Total point average of the last 100 episodes: -997.00\n",
      "Episode 1900 | Total point average of the last 100 episodes: -997.10\n",
      "Episode 2000 | Total point average of the last 100 episodes: -996.70\n",
      "\n",
      "Total Runtime: 292.87 s (4.88 min)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2_000\n",
    "max_num_timesteps = 100_000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state = env.reset()\n",
    "    state_array = np.array([\n",
    "        state[\"bird_y\"], \n",
    "        state[\"pipe_low\"][0], \n",
    "        state[\"pipe_low\"][1], \n",
    "        state[\"pipe_high\"][0], \n",
    "        state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "    is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state_array, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = get_action(q_values, epsilon)  # from utils\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward = env.game_step(action)\n",
    "        next_state_array = np.array([\n",
    "            next_state[\"bird_y\"], \n",
    "            next_state[\"pipe_low\"][0], \n",
    "            next_state[\"pipe_low\"][1], \n",
    "            next_state[\"pipe_high\"][0], \n",
    "            next_state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "        is_alive = int(state[\"is_alive\"])\n",
    "        \n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        memory_buffer.append(experience(state_array, action, reward, next_state_array, is_alive))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)  # from utils\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = get_experiences(memory_buffer)  # from utils\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        state_array = np.array([\n",
    "            state[\"bird_y\"], \n",
    "            state[\"pipe_low\"][0], \n",
    "            state[\"pipe_low\"][1], \n",
    "            state[\"pipe_high\"][0], \n",
    "            state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "        is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "        total_points += reward\n",
    "        \n",
    "        if not is_alive:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 100 points in the last 100 episodes.\n",
    "    if av_latest_points >= 100:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('flappy_bird_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points: 165\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to the initial state and get the initial state\n",
    "random.seed(42)\n",
    "state = env.reset()\n",
    "state_array = np.array([\n",
    "    state[\"bird_y\"], \n",
    "    state[\"pipe_low\"][0], \n",
    "    state[\"pipe_low\"][1], \n",
    "    state[\"pipe_high\"][0], \n",
    "    state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "total_points = 0\n",
    "\n",
    "actions = []\n",
    "\n",
    "for t in range(max_num_timesteps):\n",
    "    # From the current state S choose an action A using an ε-greedy policy\n",
    "    state_qn = np.expand_dims(state_array, axis=0)  # state needs to be the right shape for the q_network\n",
    "    q_values = q_network(state_qn)\n",
    "    action = get_action(q_values, epsilon)  # from utils\n",
    "\n",
    "    actions.append(action)\n",
    "    \n",
    "    # Take action A and receive reward R and the next state S'\n",
    "    next_state, reward = env.game_step(action)\n",
    "    next_state_array = np.array([\n",
    "        next_state[\"bird_y\"], \n",
    "        next_state[\"pipe_low\"][0], \n",
    "        next_state[\"pipe_low\"][1], \n",
    "        next_state[\"pipe_high\"][0], \n",
    "        next_state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "    is_alive = int(state[\"is_alive\"])\n",
    "    \n",
    "    state = next_state.copy()\n",
    "    state_array = np.array([\n",
    "        state[\"bird_y\"], \n",
    "        state[\"pipe_low\"][0], \n",
    "        state[\"pipe_low\"][1], \n",
    "        state[\"pipe_high\"][0], \n",
    "        state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "    is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "    if not is_alive:\n",
    "        break\n",
    "\n",
    "    if reward > 0:\n",
    "        total_points += 1\n",
    "    \n",
    "print(\"points:\", total_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "q_network.save('logs/1.fb_large_width_smol_rng.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(actions)\n",
    "\n",
    "df.to_csv(\"archive/instructions.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Reduce pipe gap\n",
    "`GAP_SIZE = 200` which initially was `300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -500.00\n",
      "Episode 200 | Total point average of the last 100 episodes: -500.00\n",
      "Episode 300 | Total point average of the last 100 episodes: -500.00\n",
      "Episode 400 | Total point average of the last 100 episodes: -500.00\n",
      "Episode 500 | Total point average of the last 100 episodes: -500.00\n",
      "Episode 600 | Total point average of the last 100 episodes: -500.00\n",
      "Episode 700 | Total point average of the last 100 episodes: -499.50\n",
      "Episode 800 | Total point average of the last 100 episodes: -492.10\n",
      "Episode 900 | Total point average of the last 100 episodes: -497.00\n",
      "Episode 1000 | Total point average of the last 100 episodes: -495.90\n",
      "Episode 1100 | Total point average of the last 100 episodes: -495.30\n",
      "Episode 1200 | Total point average of the last 100 episodes: -497.60\n",
      "Episode 1300 | Total point average of the last 100 episodes: -490.90\n",
      "Episode 1400 | Total point average of the last 100 episodes: -490.80\n",
      "Episode 1500 | Total point average of the last 100 episodes: -493.70\n",
      "Episode 1600 | Total point average of the last 100 episodes: -492.40\n",
      "Episode 1700 | Total point average of the last 100 episodes: -493.70\n",
      "Episode 1800 | Total point average of the last 100 episodes: -414.50\n",
      "Episode 1900 | Total point average of the last 100 episodes: -488.90\n",
      "Episode 2000 | Total point average of the last 100 episodes: -488.80\n",
      "\n",
      "Total Runtime: 521.40 s (8.69 min)\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import namedtuple\n",
    "\n",
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 10**(-3)          # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps\n",
    "\n",
    "env = Environment()\n",
    "\n",
    "state_size = 5  # Observation space's size\n",
    "num_actions = 2\n",
    "\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=num_actions, activation=\"linear\")    \n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=8, activation=\"relu\"),\n",
    "    Dense(units=num_actions, activation=\"linear\")\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)\n",
    "\n",
    "# q_network = load_model('logs/2.fb_large_width_smol_rng.h5')\n",
    "\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"is_alive\"])\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, is_alive = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    y_targets = rewards + is_alive * gamma * max_qsa\n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    loss = MSE(y_targets, q_values)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    update_target_network(q_network, target_q_network)\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2_000\n",
    "max_num_timesteps = 100_000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state = env.reset()\n",
    "    state_array = np.array([\n",
    "        state[\"bird_y\"], \n",
    "        state[\"pipe_low\"][0], \n",
    "        state[\"pipe_low\"][1], \n",
    "        state[\"pipe_high\"][0], \n",
    "        state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "    is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state_array, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = get_action(q_values, epsilon)  # from utils\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward = env.game_step(action)\n",
    "        next_state_array = np.array([\n",
    "            next_state[\"bird_y\"], \n",
    "            next_state[\"pipe_low\"][0], \n",
    "            next_state[\"pipe_low\"][1], \n",
    "            next_state[\"pipe_high\"][0], \n",
    "            next_state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "        is_alive = int(state[\"is_alive\"])\n",
    "        \n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        memory_buffer.append(experience(state_array, action, reward, next_state_array, is_alive))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)  # from utils\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = get_experiences(memory_buffer)  # from utils\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        state_array = np.array([\n",
    "            state[\"bird_y\"], \n",
    "            state[\"pipe_low\"][0], \n",
    "            state[\"pipe_low\"][1], \n",
    "            state[\"pipe_high\"][0], \n",
    "            state[\"pipe_high\"][1]], dtype=np.float32)\n",
    "        is_alive = int(state[\"is_alive\"])\n",
    "\n",
    "        total_points += reward\n",
    "        \n",
    "        if not is_alive:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 100 points in the last 100 episodes.\n",
    "    if av_latest_points >= 100:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('flappy_bird_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
